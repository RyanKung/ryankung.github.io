<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Ryan's Cave - 2017-06-13-convlutional-neural-networks</title>
        <script type="text/javascript" src="../js/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML-full,Safe"></script>
        <script type="text/javascript" src="https://unpkg.com/pdfjs-dist@2.0.489/build/pdf.min.js"></script>
        <script type="text/javascript" src="https://unpkg.com/pdfjs-dist@2.0.489/build/pdf.worker.min.js"></script>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/github.css" />
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">Ryan's Cave</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
            <div class="info">
    Posted on June 13, 2017
    
</div>

<h1>
<center>
Neural Networks: Convlutional and pooling
</center>
</h1>
<center>
<h4>
Ryan J. Kung
</h4>
</center>
<center>
<h4>
ryankung(at)ieee.org
<h4>
</center>
<p>CNNs are a specialized kind of neural network for parocessing data that has a known, grid-like topology. <strong>Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers</strong>[1].</p>
<h2 id="the-convolution-operation">The Convolution Operation</h2>
<p>Convolution is an mathematical operation on two real-valued functions, <span class="math inline">\(f, g\)</span>[2].</p>
<span class="math display">\[\begin{equation}
(f * g)(t) = \int_{-\infty}^{\infty}{f(\tau)g(t-\tau)d\tau}
\end{equation}\]</span>
<p>The discrete convolution can be defined as:</p>
<span class="math display">\[\begin{equation}
(f * g)(t) = \sum_{\tau=-\infty}^{\infty}{f(\tau)g(t-\tau)d\tau}
\end{equation}\]</span>
<p>In two-dimensional case, We have two-dimensional data <span class="math inline">\(I\)</span> as input, and using a two-dimensional kernel <span class="math inline">\(K\)</span>, the shape of <span class="math inline">\(I, K\)</span> is <span class="math inline">\((m, n)\)</span>:</p>
<span class="math display">\[\begin{equation}
(I * K)(i, j) = \sum_m \sum_n I(m, n)K(i-m, j-n)
\end{equation}\]</span>
<p>Convolution is commutative:</p>
<span class="math display">\[\begin{equation}
(I * K)(i, j) = \sum_m \sum_n K(i-m, j-n)I(m, n)
\end{equation}\]</span>
<p>The commutative property of convolution arises because we have <span class="math inline">\(flipped\)</span> the kernel relative to the input, in the sense that as m increased, the index into the input increases, but the index into the kernel decreases.</p>
<p>Many ML librarys implement a realted function called <span class="math inline">\(cross-correlation\)</span>, which is the same as convolution but without flipping the kernel, and call it <code>convolution</code>:</p>
<span class="math display">\[\begin{equation}
(I * K)(i, j) = \sum_m \sum_n I(i-m, j-n)K(m, n)
\end{equation}\]</span>
<p>We usually call both operations convolution and specify whether flipped the kernel. <strong>An algorithm based on convolution with kernel flipping will learn a kernel that is flipped relative to the kernel learned by an algorithm without the flipping.</strong></p>
<p>Discrete convolution can be viewd as multiplication by a matrix. However, the matrix has several entries constrained to be equal to other entries. For univariate discrete convolution, each row of the matrix is constrained to be equal to the row above shifted by one element, which is known as a <span class="math inline">\(Toeplitz\ matrix\)</span>, and in two dimensions, it’s a <span class="math inline">\(boubly \ block\ circulant\ matrix\)</span>, responsed to convolution[3].</p>
<p>Convolution usually corresponds to a very sparse matrix, this during to the kernel is usually much smaller than the input image. <strong>Any neural network algorithm that works with matrix multiplication and doses not depend on specific properties of the matrix structure should work with convolution, without requiring any futher chances to the NN.[4]</strong></p>
<h2 id="motivation">Motivation</h2>
<p>Convolution leverages three ideas that can help improve a machine learning system: <strong>sparse interactions</strong>, <strong>parameter sharing</strong> and <strong>equivariant representations</strong>.</p>
<h4 id="sparse-interactions">Sparse Interactions</h4>
<p>Instead matrix multiplication used by traditional neural network, Convolutional networks have <span class="math inline">\(sparse\ interations\)</span> (alwo referred to as <span class="math inline">\(sparse\ connectivity\)</span> or sparse weights), which is accomplished by making the kernel smaller than the input.</p>
<p>If we keep <span class="math inline">\(m\)</span> seral orders of magnitude smaller than <span class="math inline">\(m\)</span>, we can see quite large improvements.</p>
<h4 id="parameter-sharing">Parameter Sharing</h4>
<p><span class="math inline">\(Parameter\ sharing\)</span> refers to using the same parameter for more than one function in a model. In a traditional NN, each element of the weight matrix is used exactly ones. As a synonym for parameter sharing, one can say that a network has <span class="math inline">\(tied\ weights\)</span>, because the value of weight applied to one input is tied to the value of weight applied elseware.[5]</p>
<p>In CNN, each member of the kernel is used at every position of the input. The parmeter sharing used by ANN is means that, <strong>rather than learning a separate set of parameters of every location, we learn only one set</strong>. This does not affect the runtime of forward progagation – it’s still <span class="math inline">\(O(k\times n)\)</span>– but it does futher reduce the storage requirements of model to <span class="math inline">\(K\)</span> parameters which is usually several orders of magnitude less than <span class="math inline">\(m\)</span>.</p>
<p>With <span class="math inline">\(Sparse\ interactions\)</span> and <span class="math inline">\(Parmeter\ Sharing\)</span>, convolution is thus dramatically more efficient than dense matrix multiplication in terms of the memory requirements and statistical efficiency.</p>
<h4 id="equivariant-representations6">equivariant representations[6]</h4>
<p>The <em>particualar form</em> of <em>parameter sharing</em> causes the layer to have a property called <span class="math inline">\(equivariance\)</span> to translation. to say a function is equivariant means that if the input changes, the ouput changes in the same way. For example:</p>
<p><span class="math inline">\(f(x)\)</span> is equivariant to <span class="math inline">\(g\)</span> <span class="math inline">\(iff\)</span>: <span class="math inline">\(f\circ g(x) = g \circ f(x)\)</span></p>
<p>In the case of convolution, if <span class="math inline">\(g\)</span> is any function that translates the input(i.e, shift it), then the convolution function is equivariant to <span class="math inline">\(g\)</span>.</p>
<p>In the case of processing <strong>time series data</strong>, this means that convolution produces a sort of timeline that shows when different features appear in the input. If we move an event later in time in the input, the exact same representation of it will appear in the output, just later in time.</p>
<p>With the case of processing 2-D images, convolution creates a 2-D map of where certain features apear in th input. If we move the object in the input, its representation will move the same amount in the output.</p>
<h3 id="pooling">Pooling</h3>
<p>A typical layer of CNN include three stages. In the first state, the layer performs several convolutions in parallel to produce a set of linear activations. Then in the second state, each linear <em>activation</em> is run through a nonlearn activation function, such as <em>ReLU</em>, this stage is called the <span class="math inline">\(decector\ state\)</span>. In the third stage, we use a <span class="math inline">\(pooling\ function\)</span> to modify the output of the layer further.</p>
<p>A pooling function replaces the output of the net at a certain location with a summary statistic of the nearby outputs. For example, the <span class="math inline">\(max\ pooling\)</span> operation reports the maximum output within a rectangular neighborhood.</p>
<p>In all cases, pooling helps to make the representation become approximately <span class="math inline">\(invariant\)</span> to small translation of input. <strong>Invariance to local translation can be a vary useful property if we care more about whether some feature is present than exactly where it is.</strong>[7]</p>
<p>The use of pooling can ve viewed as adding an infinitely strong prior that the function the layer learns must be <strong>invariant</strong> to small translation. When this assumption is correct, it can greatly improve the statistical efficiency of the network.</p>
<p>Because pooling summarize the response over a whole neightborhood, it’s possible to use fewer pooling units than the dectector units, by reporting summary statistics for pooling regions spaced <span class="math inline">\(k\)</span> pixels apart rather than 1 pixel apart.</p>
<h2 id="reference">Reference</h2>
<p>[1][4][5][6][7] Book Deep Learning, Author Ian Gooodfellow and Yoshua Bengio and Aaron Courville, MIT Press, page 330</p>
<p>[2][3] Convolution, wikipedia https://en.wikipedia.org/wiki/Convolution</p>

        </div>

        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
