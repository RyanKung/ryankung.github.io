<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Ryan's Cave - 2017-05-26-a-deconstruct-note-of-machine-leaning-with-tensorflow</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/github.css" />
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">Ryan's Cave</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
            <div class="info">
    Posted on May 26, 2017
    
</div>

<h1>
<center>
A Deconstruct Note of Machine Leaning with Tensorflow
</center>
</h1>
<center>
<strong>Ryan J. Kung</strong>
</center>
<center>
<strong>ryankung(at)ieee.org</strong>
</center>
<h3>
<center>
Abstract
</center>
</h3>
<p>This work provide a deconstruct inside vision of tensorflow MNIST DataSet and the basic pricpile of Linear Distribution based Machine Learning.</p>
<p>As the <em>hello world</em> of Minchine Learning MNIST, we usually download it from Yann LeCun’s website (http://yann.lecun.com/exdb/mnist/), and in Tensorflow, MNIST can be easily fetched by follow codes.</p>
<p>We can check the implementation of function <code>read_data_set</code> at <a href="%60https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/mnist.py#L211%60">fn read_data_sets()</a>, which accept several parameters.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> read_data_sets(train_dir,
                   fake_data<span class="op">=</span><span class="va">False</span>,
                   one_hot<span class="op">=</span><span class="va">False</span>,
                   dtype<span class="op">=</span>dtypes.float32,
                   reshape<span class="op">=</span><span class="va">True</span>,
                   validation_size<span class="op">=</span><span class="dv">5000</span>,
                   seed<span class="op">=</span><span class="va">None</span>) <span class="op">-&gt;</span> DataSet:</code></pre></div>
<p>For the case <code>fake_data == True</code>, it will return an Fake DataSet, <em><code>DataSet([], [], fake_data=True, one_hot=one_hot, dtype=dtype, seed=seed)</code></em>, otherwise, It will returns the MNIST dataset with a high dimension dataset <code>base.Datasets(train=train, validation=validation, test=test)</code>, which include training data:label, validation data:label, and test data:label.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">
  options <span class="op">=</span> <span class="bu">dict</span>(dtype<span class="op">=</span>dtype, reshape<span class="op">=</span>reshape, seed<span class="op">=</span>seed)
  
  train <span class="op">=</span> DataSet(train_images, train_labels, <span class="op">**</span>options)
  validation <span class="op">=</span> DataSet(validation_images, validation_labels, <span class="op">**</span>options)
  test <span class="op">=</span> DataSet(test_images, test_labels, <span class="op">**</span>options)

  <span class="co"># The origin implementation is dirtier than this, there is a pull request</span>
  <span class="co"># https://github.com/tensorflow/tensorflow/pull/10188</span></code></pre></div>
<p>And it worth to notify that the <code>fake DataSet</code> is not actuall a <code>Empty DataSet</code>, the <code>fake_data</code> label parameters will cause <code>side-effect</code> of how <code>fn DataSet(fake_data=True).next_batch</code> works.</p>
<p>The above <code>one_hot</code> label means, the labels will be present as `one-hot vertaxs</p>



<p>Lets check about <span class="math inline"><em>D</em><em>a</em><em>t</em><em>a</em>(<em>i</em><em>m</em><em>a</em><em>g</em><em>e</em><sub><em>i</em></sub>)</span>, You can use <code>matplotlib</code> to display some of them.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> matplotlib <span class="im">as</span> plt
plt.imshow(np.reshape(mnist.train.images[<span class="dv">0</span>], (<span class="dv">28</span>, <span class="dv">28</span>)), cmap<span class="op">=</span><span class="st">&quot;gray&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">%</span>matplotlib inline
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> matplotlib

fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">80</span>,<span class="dv">80</span>))
<span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">31</span>):
    img <span class="op">=</span> np.reshape(train_data.images[i<span class="op">-</span><span class="dv">1</span>], (<span class="dv">28</span>, <span class="dv">28</span>))
    ax <span class="op">=</span> fig.add_subplot(<span class="dv">8</span>, <span class="dv">10</span>, i)
    matplotlib.pyplot.imshow(img, cmap<span class="op">=</span><span class="st">&quot;gray&quot;</span>)</code></pre></div>
<div class="figure">
<img src="output_12_0.png" alt="png" />
<p class="caption">png</p>
</div>
<p>All images are shuffled, and you can reshuffle them again by applying function <code>numpy.random.suffle</code> on vetaxs.</p>
<h2 id="ii-machine-learning-with-mnist-via-softmax-regressions">II Machine Learning with MNIST via Softmax Regressions</h2>
<h3 id="linear-regression">2.1 Linear Regression</h3>
<p>Now we have the input image Vertax with 28x28 dimensionals, and needs a function which can map it into a 10 dimensionals Vertax as the one-hot Vertax label. All we needs is to create a Martix with shape (10, 28*28), and a bias. Bias is often used by Term <span class="math inline"><em>l</em><em>i</em><em>n</em><em>e</em><em>a</em><em>r</em> <em>r</em><em>e</em><em>g</em><em>r</em><em>e</em><em>s</em><em>s</em><em>i</em><em>o</em><em>n</em></span> to refer to a slightly more sophisticated model[5]. Thus the function formula can be descibe as:</p>

<p>Or:</p>

<h3 id="multionoulli-distribution">2.2 Multionoulli distribution</h3>
<p><code>Softmax function</code> can be seen as a generalization of the <code>sigmoid function</code>[1], and ofen used to predict the probabilities associated with a <code>multionoulli distribution</code></p>

<p><code>Multinoulli Distribution</code> also knowns as <code>categorical distribution</code>, which is a distribution over a single discrete variable with <span class="math inline"><em>k</em></span> deifferent states, where <span class="math inline"><em>k</em></span> is finite.[2] It’s parametrized by a vector <span class="math inline"><em>P</em> ∈ [0, 1]<sup><em>k</em> − 1</sup></span>, where <span class="math inline"><em>p</em><sub><em>i</em></sub></span> gives the probability of the <span class="math inline"><em>i</em></span>-th state. In this case, <span class="math inline"><em>s</em><em>o</em><em>f</em><em>t</em><em>m</em><em>a</em><em>x</em></span> will help us to build a <code>Multinoulli Distribution</code> with a <span class="math inline"><em>k</em> = 10</span> <code>categorys</code>.</p>
<h3 id="implementing-with-tensorflow">2.3 Implementing with tensorflow</h3>
<p>Initialize Vertaxs and Matrixs First:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">x <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, <span class="dv">784</span>])</code></pre></div>
<p><span class="math inline"><em>x</em></span> is a placeholder of input images which have 28*28 dimensionals.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">W <span class="op">=</span> tf.Variable(tf.zeros([<span class="dv">784</span>, <span class="dv">10</span>]))
b <span class="op">=</span> tf.Variable(tf.zeros([<span class="dv">10</span>]))</code></pre></div>
<p>Matrix <span class="math inline"><em>W</em></span> and Vertax <span class="math inline"><em>b</em></span> are setted to all <code>Zero</code>.</p>
<p>Thus, We start to implement formule <span class="math inline"><em>f</em>(<em>x</em>)=<em>W</em><sup><em>T</em></sup><em>X</em> + <em>B</em></span> and applied with function <span class="math inline"><em>s</em><em>o</em><em>f</em><em>t</em><em>m</em><em>a</em><em>x</em></span>:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">logits <span class="op">=</span> tf.matmul(x, W) <span class="op">+</span> b
y <span class="op">=</span> tf.nn.softmax(logits)</code></pre></div>
<h3 id="training-with-maximum-likelihood-estimation">2.4 Training with Maximum Likelihood Estimation</h3>
<p>Machine Leaning is that “<span class="math inline"><em>A</em></span> computer program is said to learn from experience <span class="math inline"><em>E</em></span> with respect to some class of tasks <span class="math inline"><em>T</em></span> and performance measure <span class="math inline"><em>P</em></span>, if its performance at tasks in <span class="math inline"><em>T</em></span>, as measured by <span class="math inline"><em>P</em></span>, improves with experience <span class="math inline"><em>E</em></span> .”by Mitchell(1997).</p>
<p>So, fot training, we needs to find out a method for <span class="math inline"><em>P</em><em>e</em><em>r</em><em>f</em><em>o</em><em>r</em><em>m</em><em>a</em><em>n</em><em>c</em><em>e</em></span> measurement, which descript what is good or bad and iterate with <code>optimization</code> <span class="math inline"><em>T</em><em>a</em><em>s</em><em>k</em></span> and getter <span class="math inline"><em>E</em><em>x</em><em>p</em><em>e</em><em>r</em><em>i</em><em>e</em><em>n</em><em>c</em><em>e</em></span> via dataset. Optimization refers to the “task of either minimizing or maximizing some function <span class="math inline"><em>f</em>(<em>x</em>)</span> by altering <span class="math inline"><em>x</em></span>”.[4] We usually call the function we want to minimize or maximize as <span class="math inline"><em>c</em><em>o</em><em>s</em><em>t</em> <em>f</em><em>u</em><em>n</em><em>c</em><em>t</em><em>i</em><em>o</em><em>n</em></span>, <span class="math inline"><em>l</em><em>o</em><em>s</em><em>s</em> <em>f</em><em>u</em><em>n</em><em>c</em><em>t</em><em>i</em><em>o</em><em>n</em></span> or <span class="math inline"><em>e</em><em>r</em><em>r</em><em>o</em><em>r</em> <em>f</em><em>u</em><em>n</em><em>c</em><em>t</em><em>i</em><em>o</em><em>n</em></span>.</p>
<p>The <code>Maximum likelihood principle</code> is the most common model for making a good estimater of training models. Consider we have training DataSet $={x_{(1)}, x_{(2)}, , x_{(n)}} $, and a distribute mode <span class="math inline"><em>p</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>(<em>x</em>; <em>θ</em>)</span> which is based on <span class="math inline"><em>θ</em></span>. The Maximum Likelihood Estimator is defined as:</p>

Where:

<p>And we can simply and equivalent trans the product function with sum function:</p>

<p>Ony way to interpret maxium likelihood estimation is to view it as minimizing the dissimilarity between the empirical distribution <span class="math inline"><em>P</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub></span> defined by the training set and the model distribution, weith the degreee of dissimilarity between the two measured by the <span class="math inline"><em>K</em><em>L</em></span> divergence.</p>

<p>Minimizing this KL divergence corresponds exactly to minimizing the <code>cross-entropy</code> between the distributions.[6]</p>
To estimate a conditional probability <span class="math inline"><em>P</em>(<em>y</em>|<em>x</em>; /<em>t</em><em>h</em><em>e</em><em>t</em><em>a</em>)</span>, If <span class="math inline"><em>X</em></span> presents all out inputs and <span class="math inline"><em>Y</em></span> all out oversved targets, then the CML is:

<p>or</p>

<p>as the documents of tensorflow[4].</p>
<h3 id="cross-entropy-traing-with-tensorflow">2.5 Cross-entropy traing with tensorflow</h3>
<p>For cross entropy implementation with Tensorflow, it only needs two lins of code:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">y_ <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, <span class="dv">10</span>])
cross_entropy <span class="op">=</span> tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits<span class="op">=</span>logits, labels<span class="op">=</span>y_))</code></pre></div>
<p>Then we choosed Grandient Descent Optimizer[7] for minimize cross_entropy:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">train_step <span class="op">=</span> tf.train.GradientDescentOptimizer(<span class="fl">0.5</span>).minimize(cross_entropy)</code></pre></div>
<h3 id="go-train">2.6 Go Train</h3>
<p>Before training start, you may needs to check your hardware resource by code</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> tensorflow.python.client <span class="im">import</span> device_lib
device_lib.list_local_devices()</code></pre></div>
<p>First launch the model session with GPU.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">gpu_options <span class="op">=</span> tf.GPUOptions(per_process_gpu_memory_fraction<span class="op">=</span><span class="fl">0.9</span>)
config<span class="op">=</span>tf.ConfigProto(gpu_options<span class="op">=</span>gpu_options, log_device_placement<span class="op">=</span><span class="va">True</span>)
session <span class="op">=</span> tf.InteractiveSession(config<span class="op">=</span>config)</code></pre></div>
<p>Then we’ll run the training step 1000 times.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">config<span class="op">=</span>tf.ConfigProto(gpu_options<span class="op">=</span>gpu_options, log_device_placement<span class="op">=</span><span class="va">True</span>)
sess <span class="op">=</span> tf.InteractiveSession(config<span class="op">=</span>config)
tf.global_variables_initializer().run()

<span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):
    batch_xs, batch_ys <span class="op">=</span> mnist.train.next_batch(<span class="dv">100</span>)
    sess.run(train_step, feed_dict<span class="op">=</span>{x: batch_xs, y_: batch_ys})</code></pre></div>
<p>And Evaluating via below codes:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">correct_prediction <span class="op">=</span> tf.equal(tf.argmax(y,<span class="dv">1</span>), tf.argmax(y_,<span class="dv">1</span>))
accuracy <span class="op">=</span> tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
<span class="bu">print</span>(sess.run(accuracy, feed_dict<span class="op">=</span>{x: mnist.test.images, y_: mnist.test.labels}))</code></pre></div>
<h2 id="reference">Reference</h2>
<p>[1][2][4][5][6] Book Deep Learning, Author Ian Gooodfellow and Yoshua Bengio and Aaron Courville, MIT Press, page 198</p>
<p>[3][4] MNIST For ML Beginners https://www.tensorflow.org/get_started/mnist/beginners</p>
<p>[7]Machine Learing MOOC, Andrew Ng. https://www.coursera.org/learn/machine-learning/exam/wjqip/introduction</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"></code></pre></div>

        </div>

        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
